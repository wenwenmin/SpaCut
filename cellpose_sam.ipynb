{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e674615d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Welcome to CellposeSAM, cellpose v\n",
      "cellpose version: \t4.0.4 \n",
      "platform:       \twin32 \n",
      "python version: \t3.10.18 \n",
      "torch version:  \t2.5.0+cu124! The neural network component of\n",
      "CPSAM is much larger than in previous versions and CPU excution is slow. \n",
      "We encourage users to use GPU/MPS if available. \n",
      "\n",
      "\n",
      "2025-10-22 22:00:56,491 [INFO] WRITING LOG OUTPUT TO C:\\Users\\lyh\\.cellpose\\run.log\n",
      "2025-10-22 22:00:56,494 [INFO] \n",
      "cellpose version: \t4.0.4 \n",
      "platform:       \twin32 \n",
      "python version: \t3.10.18 \n",
      "torch version:  \t2.5.0+cu124\n",
      "2025-10-22 22:00:56,598 [INFO] ** TORCH CUDA version installed and working. **\n",
      "2025-10-22 22:00:56,599 [INFO] >>>> using GPU (CUDA)\n",
      "2025-10-22 22:00:57,814 [INFO] >>>> loading model C:\\Users\\lyh\\.cellpose\\models\\cpsam\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from cellpose import models, core, io, plot\n",
    "from pathlib import Path\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "from natsort import natsorted\n",
    "\n",
    "io.logger_setup() # run this to get printing of progress\n",
    "\n",
    "model = models.CellposeModel(gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aa896c",
   "metadata": {},
   "source": [
    "#### Xenium FFPE Human Breast Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf10370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellpose-SAM segmentation for Xenium FFPE Human Breast Cancer DAPI images\n",
    "import os\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "from cellpose import models, core, io, plot\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from natsort import natsorted\n",
    "\n",
    "# Set up logging\n",
    "io.logger_setup()\n",
    "\n",
    "# DAPI image path - Xenium FFPE Human Breast Cancer\n",
    "dapi_path = \"D:/segmentation_datasets/Xenium_FFPE_Human_Breast_Cancer_Rep1_outs/outs/morphology_mip.ome.tif\"\n",
    "\n",
    "\n",
    "# Load DAPI image\n",
    "print(f\"Loading DAPI image: {dapi_path}\")\n",
    "if not os.path.exists(dapi_path):\n",
    "    print(f\"Error: DAPI image file not found: {dapi_path}\")\n",
    "    exit()\n",
    "\n",
    "dapi_img = tifffile.imread(dapi_path)\n",
    "print(f\"Original image shape: {dapi_img.shape}\")\n",
    "print(f\"Original image dtype: {dapi_img.dtype}, max: {dapi_img.max()}, min: {dapi_img.min()}\")\n",
    "\n",
    "# Image preprocessing - handle possible multi-dimensional images\n",
    "if len(dapi_img.shape) == 3:\n",
    "    # If 3D image (possibly Z, Y, X or Y, X, C)\n",
    "    if dapi_img.shape[0] < dapi_img.shape[1]:  # Channel might be in the first dimension\n",
    "        # Assume (C, H, W) format, convert to (H, W, C)\n",
    "        dapi_img = np.transpose(dapi_img, (1, 2, 0))\n",
    "        print(f\"Converted image format to (H, W, C): {dapi_img.shape}\")\n",
    "    # If multiple Z layers, take maximum projection or middle layer\n",
    "    if dapi_img.shape[-1] > 3:  # If too many channels, might be Z layers\n",
    "        print(\"Detected multi-layer image, using maximum intensity projection\")\n",
    "        dapi_img = np.max(dapi_img, axis=-1, keepdims=True)\n",
    "elif len(dapi_img.shape) == 2:\n",
    "    # If 2D image, convert to 3D (H, W, C)\n",
    "    dapi_img = np.expand_dims(dapi_img, axis=-1)\n",
    "    print(f\"Converted image shape: {dapi_img.shape}\")\n",
    "\n",
    "# Image normalization\n",
    "if dapi_img.max() > 0:\n",
    "    if dapi_img.dtype == np.uint16:\n",
    "        print(\"Image is uint16 type, keeping original format\")\n",
    "        dapi_img_norm = dapi_img.copy()\n",
    "    elif dapi_img.max() > 255:\n",
    "        print(\"Normalizing image to 0-255 (uint8)\")\n",
    "        dapi_img_norm = (dapi_img / dapi_img.max() * 255).astype(np.uint8)\n",
    "    else:\n",
    "        print(\"Image is already in suitable intensity range\")\n",
    "        dapi_img_norm = dapi_img.astype(np.uint8)\n",
    "else:\n",
    "    print(\"Warning: Image max value is 0, might be empty image\")\n",
    "    dapi_img_norm = dapi_img.astype(np.uint8)\n",
    "\n",
    "# Channel selection - according to new Cellpose format\n",
    "# For DAPI single-channel image, we use the first channel\n",
    "first_channel = '0'   # Use the first channel (DAPI)\n",
    "second_channel = 'None'  # No second channel\n",
    "third_channel = 'None'   # No third channel\n",
    "\n",
    "selected_channels = []\n",
    "for i, c in enumerate([first_channel, second_channel, third_channel]):\n",
    "    if c == 'None':\n",
    "        continue\n",
    "    if int(c) >= dapi_img_norm.shape[-1]:  # Fixed boundary check\n",
    "        print(f\"Warning: Channel index {c} exceeds image channel count {dapi_img_norm.shape[-1]}\")\n",
    "        continue\n",
    "    if c != 'None':\n",
    "        selected_channels.append(int(c))\n",
    "\n",
    "print(f\"Selected channels: {selected_channels}\")\n",
    "\n",
    "# Create image with selected channels\n",
    "img_selected_channels = np.zeros_like(dapi_img_norm)\n",
    "if len(selected_channels) > 0:\n",
    "    img_selected_channels[:, :, :len(selected_channels)] = dapi_img_norm[:, :, selected_channels]\n",
    "else:\n",
    "    # If no channels selected, use original image\n",
    "    img_selected_channels = dapi_img_norm\n",
    "\n",
    "print(f\"Processed image shape: {img_selected_channels.shape}\")\n",
    "\n",
    "# Initialize new version of CellposeModel (default using cpsam model)\n",
    "print(\"Initializing CellposeModel (using default cpsam model)...\")\n",
    "model = models.CellposeModel(gpu=True)  # Default pretrained_model=\"cpsam\"\n",
    "\n",
    "# Set segmentation parameters - optimized for Xenium data\n",
    "flow_threshold = 0.4     # Flow threshold\n",
    "cellprob_threshold = 0.0 # Cell probability threshold\n",
    "tile_norm_blocksize = 0  # Tile normalization block size, 0 means no tile normalization\n",
    "batch_size = 8           # Reduce batch size to fit large images\n",
    "\n",
    "print(f\"Running Cellpose-SAM segmentation with the following parameters:\")\n",
    "print(f\"- pretrained_model: cpsam (default)\")\n",
    "print(f\"- flow_threshold: {flow_threshold}\")\n",
    "print(f\"- cellprob_threshold: {cellprob_threshold}\")\n",
    "print(f\"- tile_norm_blocksize: {tile_norm_blocksize}\")\n",
    "print(f\"- batch_size: {batch_size}\")\n",
    "\n",
    "# Run new version of Cellpose-SAM segmentation\n",
    "masks, flows, styles = model.eval(\n",
    "    img_selected_channels, \n",
    "    batch_size=batch_size, \n",
    "    flow_threshold=flow_threshold, \n",
    "    cellprob_threshold=cellprob_threshold,\n",
    "    normalize={\"tile_norm_blocksize\": tile_norm_blocksize}\n",
    ")\n",
    "\n",
    "print(f\"Segmentation complete. Found {masks.max()} cells.\")\n",
    "\n",
    "output_dir  = \"cellpose_sam_segmentation\"\n",
    "# Save segmentation results\n",
    "mask_filename = \"cellpose_sam_masks_xenium_breast_cancer.tif\"\n",
    "mask_path = os.path.join(output_dir, mask_filename)\n",
    "tifffile.imwrite(mask_path, masks.astype(np.uint32))\n",
    "print(f\"Mask file saved: {mask_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d041bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89f971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2855b2a1",
   "metadata": {},
   "source": [
    "#### stereo seq sub image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf45fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellpose-SAM segmentation for Stereo-seq Mouse Brain data\n",
    "import os\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "from cellpose import models, core, io, plot\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from natsort import natsorted\n",
    "\n",
    "# Set up logging\n",
    "io.logger_setup()\n",
    "\n",
    "# Stereo-seq image path - Mouse Brain Adult\n",
    "dapi_path = \"D:/paper/newmodel/data/stereo_seq_mouse_brain/Mouse_brain_Adult_sub.tif\"\n",
    "\n",
    "# dapi_path = r\"D:\\paper\\newmodel\\data\\NanoString_CosMx_Human_Pancreas\\fov51\\DAPI_F051.tif\"\n",
    "\n",
    "# Output directory for segmentation results\n",
    "output_dir_base = r\"D:/paper/newmodel/data/stereo_seq_mouse_brain/cellpose_sam_segmentation\"\n",
    "\n",
    "output_dir = os.path.join(output_dir_base, \"cellpose_sam_segmentation\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load Stereo-seq image\n",
    "print(f\"Loading Stereo-seq image: {dapi_path}\")\n",
    "if not os.path.exists(dapi_path):\n",
    "    print(f\"Error: Stereo-seq image file not found: {dapi_path}\")\n",
    "    exit()\n",
    "\n",
    "dapi_img = tifffile.imread(dapi_path)\n",
    "print(f\"Original image shape: {dapi_img.shape}\")\n",
    "print(f\"Original image dtype: {dapi_img.dtype}, max: {dapi_img.max()}, min: {dapi_img.min()}\")\n",
    "\n",
    "# Image preprocessing - handle possible multi-dimensional images\n",
    "if len(dapi_img.shape) == 3:\n",
    "    # If 3D image (possibly Z, Y, X or Y, X, C)\n",
    "    if dapi_img.shape[0] < dapi_img.shape[1]:  # Channel might be in the first dimension\n",
    "        # Assume (C, H, W) format, convert to (H, W, C)\n",
    "        dapi_img = np.transpose(dapi_img, (1, 2, 0))\n",
    "        print(f\"Converted image format to (H, W, C): {dapi_img.shape}\")\n",
    "    # If multiple Z layers, take maximum projection or middle layer\n",
    "    if dapi_img.shape[-1] > 3:  # If too many channels, might be Z layers\n",
    "        print(\"Detected multi-layer image, using maximum intensity projection\")\n",
    "        dapi_img = np.max(dapi_img, axis=-1, keepdims=True)\n",
    "elif len(dapi_img.shape) == 2:\n",
    "    # If 2D image, convert to 3D (H, W, C)\n",
    "    dapi_img = np.expand_dims(dapi_img, axis=-1)\n",
    "    print(f\"Converted image shape: {dapi_img.shape}\")\n",
    "\n",
    "# Image normalization\n",
    "if dapi_img.max() > 0:\n",
    "    if dapi_img.dtype == np.uint16:\n",
    "        print(\"Image is uint16 type, keeping original format\")\n",
    "        dapi_img_norm = dapi_img.copy()\n",
    "    elif dapi_img.max() > 255:\n",
    "        print(\"Normalizing image to 0-255 (uint8)\")\n",
    "        dapi_img_norm = (dapi_img / dapi_img.max() * 255).astype(np.uint8)\n",
    "    else:\n",
    "        print(\"Image is already in suitable intensity range\")\n",
    "        dapi_img_norm = dapi_img.astype(np.uint8)\n",
    "else:\n",
    "    print(\"Warning: Image max value is 0, might be empty image\")\n",
    "    dapi_img_norm = dapi_img.astype(np.uint8)\n",
    "\n",
    "# Channel selection - according to new Cellpose format\n",
    "# For Stereo-seq single-channel image, we use the first channel\n",
    "first_channel = '0'   # Use the first channel\n",
    "second_channel = 'None'  # No second channel\n",
    "third_channel = 'None'   # No third channel\n",
    "\n",
    "selected_channels = []\n",
    "for i, c in enumerate([first_channel, second_channel, third_channel]):\n",
    "    if c == 'None':\n",
    "        continue\n",
    "    if int(c) >= dapi_img_norm.shape[-1]:  # Fixed boundary check\n",
    "        print(f\"Warning: Channel index {c} exceeds image channel count {dapi_img_norm.shape[-1]}\")\n",
    "        continue\n",
    "    if c != 'None':\n",
    "        selected_channels.append(int(c))\n",
    "\n",
    "print(f\"Selected channels: {selected_channels}\")\n",
    "\n",
    "# Create image with selected channels\n",
    "img_selected_channels = np.zeros_like(dapi_img_norm)\n",
    "if len(selected_channels) > 0:\n",
    "    img_selected_channels[:, :, :len(selected_channels)] = dapi_img_norm[:, :, selected_channels]\n",
    "else:\n",
    "    # If no channels selected, use original image\n",
    "    img_selected_channels = dapi_img_norm\n",
    "\n",
    "print(f\"Processed image shape: {img_selected_channels.shape}\")\n",
    "\n",
    "# Initialize new version of CellposeModel (default using cpsam model)\n",
    "print(\"Initializing CellposeModel (using default cpsam model)...\")\n",
    "model = models.CellposeModel(gpu=True)  # Default pretrained_model=\"cpsam\"\n",
    "\n",
    "# Set segmentation parameters - optimized for Stereo-seq data\n",
    "flow_threshold = 0.4     # Flow threshold\n",
    "cellprob_threshold = 0.0 # Cell probability threshold\n",
    "tile_norm_blocksize = 0  # Tile normalization block size, 0 means no tile normalization\n",
    "batch_size = 8           # Reduce batch size to fit large images\n",
    "\n",
    "print(f\"Running Cellpose-SAM segmentation with the following parameters:\")\n",
    "print(f\"- pretrained_model: cpsam (default)\")\n",
    "print(f\"- flow_threshold: {flow_threshold}\")\n",
    "print(f\"- cellprob_threshold: {cellprob_threshold}\")\n",
    "print(f\"- tile_norm_blocksize: {tile_norm_blocksize}\")\n",
    "print(f\"- batch_size: {batch_size}\")\n",
    "\n",
    "# Run new version of Cellpose-SAM segmentation\n",
    "masks, flows, styles = model.eval(\n",
    "    img_selected_channels, \n",
    "    batch_size=batch_size, \n",
    "    flow_threshold=flow_threshold, \n",
    "    cellprob_threshold=cellprob_threshold,\n",
    "    normalize={\"tile_norm_blocksize\": tile_norm_blocksize}\n",
    ")\n",
    "\n",
    "print(f\"Segmentation complete. Found {masks.max()} cells.\")\n",
    "\n",
    "# Save segmentation results\n",
    "mask_filename = \"cellpose_sam_masks_stereo_seq_mouse_brain.tif\"\n",
    "mask_path = os.path.join(output_dir, mask_filename)\n",
    "tifffile.imwrite(mask_path, masks.astype(np.uint32))\n",
    "print(f\"Mask file saved: {mask_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c935f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94efb7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb51165f",
   "metadata": {},
   "source": [
    "#### Xenium lung cancer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a37e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating new log file\n",
      "2025-10-22 22:00:58,690 [INFO] WRITING LOG OUTPUT TO C:\\Users\\lyh\\.cellpose\\run.log\n",
      "2025-10-22 22:00:58,691 [INFO] \n",
      "cellpose version: \t4.0.4 \n",
      "platform:       \twin32 \n",
      "python version: \t3.10.18 \n",
      "torch version:  \t2.5.0+cu124\n",
      "原始图像形状: (17098, 51187)\n",
      "原始图像数据类型: uint16, 最大值: 10866, 最小值: 0\n",
      "转换后图像形状: (17098, 51187, 1)\n",
      "图像是 uint16 类型，保持原始格式\n",
      "选择的通道: [0]\n",
      "处理后图像形状: (17098, 51187, 1)\n",
      "初始化 CellposeModel (使用默认的 cpsam 模型)...\n",
      "2025-10-22 22:01:00,881 [INFO] ** TORCH CUDA version installed and working. **\n",
      "2025-10-22 22:01:00,881 [INFO] >>>> using GPU (CUDA)\n",
      "2025-10-22 22:01:02,039 [INFO] >>>> loading model C:\\Users\\lyh\\.cellpose\\models\\cpsam\n",
      "运行 Cellpose-SAM 分割，参数如下:\n",
      "- pretrained_model: cpsam (默认)\n",
      "- flow_threshold: 0.4\n",
      "- cellprob_threshold: 0.0\n",
      "- tile_norm_blocksize: 0\n",
      "- batch_size: 8\n",
      "2025-10-22 22:18:01,591 [WARNING] WARNING: image is very large, not using gpu to compute flows from masks for QC step flow_threshold\n",
      "2025-10-22 22:18:01,607 [INFO] turn off QC step with flow_threshold=0 if too slow\n",
      "2025-10-22 23:33:41,418 [WARNING] more than 65535 masks in image, masks returned as np.uint32\n",
      "分割完成。找到 148871 个细胞。\n",
      "掩码文件已保存: C:/Users/lyh/paper/image_baseline_stitch/Xenium_human_Lung_Cancer_exp2/cellpose_sam_segmentation\\cellpose_sam_masks_Xenium_human_Lung_Cancer.tif\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "from cellpose import models, core, io, plot\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from natsort import natsorted\n",
    "\n",
    "# Set up logging\n",
    "io.logger_setup()\n",
    "\n",
    "\n",
    "dapi_path = \"C:/Users/lyh/paper/image_baseline_stitch/Xenium_human_Lung_Cancer/dapi/dapi_channel.tif\"\n",
    "\n",
    "# Output directory for segmentation results\n",
    "output_dir_base = \"C:/Users/lyh/paper/image_baseline_stitch/Xenium_human_Lung_Cancer_exp2/\"\n",
    "output_dir = os.path.join(output_dir_base, \"cellpose_sam_segmentation\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "dapi_img = tifffile.imread(dapi_path)\n",
    "print(f\"Original image shape: {dapi_img.shape}\")\n",
    "print(f\"Original image dtype: {dapi_img.dtype}, max: {dapi_img.max()}, min: {dapi_img.min()}\")\n",
    "\n",
    "# Image preprocessing - handle possible multi-dimensional images\n",
    "if len(dapi_img.shape) == 3:\n",
    "    # If 3D image (possibly Z, Y, X or Y, X, C)\n",
    "    if dapi_img.shape[0] < dapi_img.shape[1]:  # Channel might be in the first dimension\n",
    "        # Assume (C, H, W) format, convert to (H, W, C)\n",
    "        dapi_img = np.transpose(dapi_img, (1, 2, 0))\n",
    "        print(f\"Converted image format to (H, W, C): {dapi_img.shape}\")\n",
    "    # If multiple Z layers, take maximum projection or middle layer\n",
    "    if dapi_img.shape[-1] > 3:  # If too many channels, might be Z layers\n",
    "        print(\"Detected multi-layer image, using maximum intensity projection\")\n",
    "        dapi_img = np.max(dapi_img, axis=-1, keepdims=True)\n",
    "elif len(dapi_img.shape) == 2:\n",
    "    # If 2D image, convert to 3D (H, W, C)\n",
    "    dapi_img = np.expand_dims(dapi_img, axis=-1)\n",
    "    print(f\"Converted image shape: {dapi_img.shape}\")\n",
    "\n",
    "# Image normalization\n",
    "if dapi_img.max() > 0:\n",
    "    if dapi_img.dtype == np.uint16:\n",
    "        print(\"Image is uint16 type, keeping original format\")\n",
    "        dapi_img_norm = dapi_img.copy()\n",
    "    elif dapi_img.max() > 255:\n",
    "        print(\"Normalizing image to 0-255 (uint8)\")\n",
    "        dapi_img_norm = (dapi_img / dapi_img.max() * 255).astype(np.uint8)\n",
    "    else:\n",
    "        print(\"Image is already in suitable intensity range\")\n",
    "        dapi_img_norm = dapi_img.astype(np.uint8)\n",
    "else:\n",
    "    print(\"Warning: Image max value is 0, might be empty image\")\n",
    "    dapi_img_norm = dapi_img.astype(np.uint8)\n",
    "\n",
    "# Channel selection - according to new Cellpose format\n",
    "# For single-channel image, we use the first channel\n",
    "first_channel = '0'   # Use the first channel\n",
    "second_channel = 'None'  # No second channel\n",
    "third_channel = 'None'   # No third channel\n",
    "\n",
    "selected_channels = []\n",
    "for i, c in enumerate([first_channel, second_channel, third_channel]):\n",
    "    if c == 'None':\n",
    "        continue\n",
    "    if int(c) >= dapi_img_norm.shape[-1]:  # Fixed boundary check\n",
    "        print(f\"Warning: Channel index {c} exceeds image channel count {dapi_img_norm.shape[-1]}\")\n",
    "        continue\n",
    "    if c != 'None':\n",
    "        selected_channels.append(int(c))\n",
    "\n",
    "print(f\"Selected channels: {selected_channels}\")\n",
    "\n",
    "# Create image with selected channels\n",
    "img_selected_channels = np.zeros_like(dapi_img_norm)\n",
    "if len(selected_channels) > 0:\n",
    "    img_selected_channels[:, :, :len(selected_channels)] = dapi_img_norm[:, :, selected_channels]\n",
    "else:\n",
    "    # If no channels selected, use original image\n",
    "    img_selected_channels = dapi_img_norm\n",
    "\n",
    "print(f\"Processed image shape: {img_selected_channels.shape}\")\n",
    "\n",
    "# Initialize new version of CellposeModel (default using cpsam model)\n",
    "print(\"Initializing CellposeModel (using default cpsam model)...\")\n",
    "model = models.CellposeModel(gpu=True)  # Default pretrained_model=\"cpsam\"\n",
    "\n",
    "# Set segmentation parameters - optimized for Xenium data\n",
    "flow_threshold = 0.4     # Flow threshold\n",
    "cellprob_threshold = 0.0 # Cell probability threshold\n",
    "tile_norm_blocksize = 0  # Tile normalization block size, 0 means no tile normalization\n",
    "batch_size = 8           # Reduce batch size to fit large images\n",
    "\n",
    "print(f\"Running Cellpose-SAM segmentation with the following parameters:\")\n",
    "print(f\"- pretrained_model: cpsam (default)\")\n",
    "print(f\"- flow_threshold: {flow_threshold}\")\n",
    "print(f\"- cellprob_threshold: {cellprob_threshold}\")\n",
    "print(f\"- tile_norm_blocksize: {tile_norm_blocksize}\")\n",
    "print(f\"- batch_size: {batch_size}\")\n",
    "\n",
    "# Run new version of Cellpose-SAM segmentation\n",
    "masks, flows, styles = model.eval(\n",
    "    img_selected_channels, \n",
    "    batch_size=batch_size, \n",
    "    flow_threshold=flow_threshold, \n",
    "    cellprob_threshold=cellprob_threshold,\n",
    "    normalize={\"tile_norm_blocksize\": tile_norm_blocksize}\n",
    ")\n",
    "\n",
    "print(f\"Segmentation complete. Found {masks.max()} cells.\")\n",
    "\n",
    "# Save segmentation results\n",
    "mask_filename = \"cellpose_sam_masks_Xenium_human_Lung_Cancer.tif\"\n",
    "mask_path = os.path.join(output_dir, mask_filename)\n",
    "tifffile.imwrite(mask_path, masks.astype(np.uint32))\n",
    "print(f\"Mask file saved: {mask_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9763747a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99010328",
   "metadata": {},
   "source": [
    "#### Xenium_FFPE_Human_Pancreas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35f2694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating new log file\n",
      "2025-10-22 23:34:34,153 [INFO] WRITING LOG OUTPUT TO C:\\Users\\lyh\\.cellpose\\run.log\n",
      "2025-10-22 23:34:34,153 [INFO] \n",
      "cellpose version: \t4.0.4 \n",
      "platform:       \twin32 \n",
      "python version: \t3.10.18 \n",
      "torch version:  \t2.5.0+cu124\n",
      "原始图像形状: (13770, 34155)\n",
      "原始图像数据类型: uint16, 最大值: 10002, 最小值: 0\n",
      "转换后图像形状: (13770, 34155, 1)\n",
      "图像是 uint16 类型，保持原始格式\n",
      "选择的通道: [0]\n",
      "处理后图像形状: (13770, 34155, 1)\n",
      "初始化 CellposeModel (使用默认的 cpsam 模型)...\n",
      "2025-10-22 23:34:35,629 [INFO] ** TORCH CUDA version installed and working. **\n",
      "2025-10-22 23:34:35,629 [INFO] >>>> using GPU (CUDA)\n",
      "2025-10-22 23:34:36,979 [INFO] >>>> loading model C:\\Users\\lyh\\.cellpose\\models\\cpsam\n",
      "运行 Cellpose-SAM 分割，参数如下:\n",
      "- pretrained_model: cpsam (默认)\n",
      "- flow_threshold: 0.4\n",
      "- cellprob_threshold: 0.0\n",
      "- tile_norm_blocksize: 0\n",
      "- batch_size: 8\n",
      "2025-10-23 00:06:37,423 [WARNING] more than 65535 masks in image, masks returned as np.uint32\n",
      "分割完成。找到 136361 个细胞。\n",
      "掩码文件已保存: C:/Users/lyh/paper/image_baseline_stitch/Xenium_FFPE_Human_Pancreas_exp1\\cellpose_sam_segmentation\\cellpose_sam_masks_Xenium_FFPE_Human_Pancreas.tif\n"
     ]
    }
   ],
   "source": [
    " \n",
    "import os\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "from cellpose import models, core, io, plot\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from natsort import natsorted\n",
    "\n",
    "# Set up logging\n",
    "io.logger_setup()\n",
    "\n",
    "\n",
    "dapi_path = \"C:/Users/lyh/paper/image_baseline_stitch/Xenium_FFPE_Human_Pancreas/dapi/dapi_channel.tif\"\n",
    "\n",
    "# Output directory for segmentation results\n",
    "output_dir_base = \"C:/Users/lyh/paper/image_baseline_stitch/Xenium_FFPE_Human_Pancreas_exp1\"\n",
    "output_dir = os.path.join(output_dir_base, \"cellpose_sam_segmentation\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "dapi_img = tifffile.imread(dapi_path)\n",
    "print(f\"Original image shape: {dapi_img.shape}\")\n",
    "print(f\"Original image dtype: {dapi_img.dtype}, max: {dapi_img.max()}, min: {dapi_img.min()}\")\n",
    "\n",
    "# Image preprocessing - handle possible multi-dimensional images\n",
    "if len(dapi_img.shape) == 3:\n",
    "    # If 3D image (possibly Z, Y, X or Y, X, C)\n",
    "    if dapi_img.shape[0] < dapi_img.shape[1]:  # Channel might be in the first dimension\n",
    "        # Assume (C, H, W) format, convert to (H, W, C)\n",
    "        dapi_img = np.transpose(dapi_img, (1, 2, 0))\n",
    "        print(f\"Converted image format to (H, W, C): {dapi_img.shape}\")\n",
    "    # If multiple Z layers, take maximum projection or middle layer\n",
    "    if dapi_img.shape[-1] > 3:  # If too many channels, might be Z layers\n",
    "        print(\"Detected multi-layer image, using maximum intensity projection\")\n",
    "        dapi_img = np.max(dapi_img, axis=-1, keepdims=True)\n",
    "elif len(dapi_img.shape) == 2:\n",
    "    # If 2D image, convert to 3D (H, W, C)\n",
    "    dapi_img = np.expand_dims(dapi_img, axis=-1)\n",
    "    print(f\"Converted image shape: {dapi_img.shape}\")\n",
    "\n",
    "# Image normalization\n",
    "if dapi_img.max() > 0:\n",
    "    if dapi_img.dtype == np.uint16:\n",
    "        print(\"Image is uint16 type, keeping original format\")\n",
    "        dapi_img_norm = dapi_img.copy()\n",
    "    elif dapi_img.max() > 255:\n",
    "        print(\"Normalizing image to 0-255 (uint8)\")\n",
    "        dapi_img_norm = (dapi_img / dapi_img.max() * 255).astype(np.uint8)\n",
    "    else:\n",
    "        print(\"Image is already in suitable intensity range\")\n",
    "        dapi_img_norm = dapi_img.astype(np.uint8)\n",
    "else:\n",
    "    print(\"Warning: Image max value is 0, might be empty image\")\n",
    "    dapi_img_norm = dapi_img.astype(np.uint8)\n",
    "\n",
    "# Channel selection - according to new Cellpose format\n",
    "# For single-channel image, we use the first channel\n",
    "first_channel = '0'   # Use the first channel\n",
    "second_channel = 'None'  # No second channel\n",
    "third_channel = 'None'   # No third channel\n",
    "\n",
    "selected_channels = []\n",
    "for i, c in enumerate([first_channel, second_channel, third_channel]):\n",
    "    if c == 'None':\n",
    "        continue\n",
    "    if int(c) >= dapi_img_norm.shape[-1]:  # Fixed boundary check\n",
    "        print(f\"Warning: Channel index {c} exceeds image channel count {dapi_img_norm.shape[-1]}\")\n",
    "        continue\n",
    "    if c != 'None':\n",
    "        selected_channels.append(int(c))\n",
    "\n",
    "print(f\"Selected channels: {selected_channels}\")\n",
    "\n",
    "# Create image with selected channels\n",
    "img_selected_channels = np.zeros_like(dapi_img_norm)\n",
    "if len(selected_channels) > 0:\n",
    "    img_selected_channels[:, :, :len(selected_channels)] = dapi_img_norm[:, :, selected_channels]\n",
    "else:\n",
    "    # If no channels selected, use original image\n",
    "    img_selected_channels = dapi_img_norm\n",
    "\n",
    "print(f\"Processed image shape: {img_selected_channels.shape}\")\n",
    "\n",
    "# Initialize new version of CellposeModel (default using cpsam model)\n",
    "print(\"Initializing CellposeModel (using default cpsam model)...\")\n",
    "model = models.CellposeModel(gpu=True)  # Default pretrained_model=\"cpsam\"\n",
    "\n",
    "# Set segmentation parameters - optimized for Xenium data\n",
    "flow_threshold = 0.4     # Flow threshold\n",
    "cellprob_threshold = 0.0 # Cell probability threshold\n",
    "tile_norm_blocksize = 0  # Tile normalization block size, 0 means no tile normalization\n",
    "batch_size = 8           # Reduce batch size to fit large images\n",
    "\n",
    "print(f\"Running Cellpose-SAM segmentation with the following parameters:\")\n",
    "print(f\"- pretrained_model: cpsam (default)\")\n",
    "print(f\"- flow_threshold: {flow_threshold}\")\n",
    "print(f\"- cellprob_threshold: {cellprob_threshold}\")\n",
    "print(f\"- tile_norm_blocksize: {tile_norm_blocksize}\")\n",
    "print(f\"- batch_size: {batch_size}\")\n",
    "\n",
    "# Run new version of Cellpose-SAM segmentation\n",
    "masks, flows, styles = model.eval(\n",
    "    img_selected_channels, \n",
    "    batch_size=batch_size, \n",
    "    flow_threshold=flow_threshold, \n",
    "    cellprob_threshold=cellprob_threshold,\n",
    "    normalize={\"tile_norm_blocksize\": tile_norm_blocksize}\n",
    ")\n",
    "\n",
    "print(f\"Segmentation complete. Found {masks.max()} cells.\")\n",
    "\n",
    "# Save segmentation results\n",
    "mask_filename = \"cellpose_sam_masks_Xenium_FFPE_Human_Pancreas.tif\"\n",
    "mask_path = os.path.join(output_dir, mask_filename)\n",
    "tifffile.imwrite(mask_path, masks.astype(np.uint32))\n",
    "print(f\"Mask file saved: {mask_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a632f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "237cf436",
   "metadata": {},
   "source": [
    "#### Xenium_V1_mouse_Colon_FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb302e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Welcome to CellposeSAM, cellpose v\n",
      "cellpose version: \t4.0.4 \n",
      "platform:       \twin32 \n",
      "python version: \t3.10.18 \n",
      "torch version:  \t2.5.0+cu124! The neural network component of\n",
      "CPSAM is much larger than in previous versions and CPU excution is slow. \n",
      "We encourage users to use GPU/MPS if available. \n",
      "\n",
      "\n",
      "2025-10-23 22:25:31,640 [INFO] WRITING LOG OUTPUT TO C:\\Users\\lyh\\.cellpose\\run.log\n",
      "2025-10-23 22:25:31,640 [INFO] \n",
      "cellpose version: \t4.0.4 \n",
      "platform:       \twin32 \n",
      "python version: \t3.10.18 \n",
      "torch version:  \t2.5.0+cu124\n",
      "原始图像形状: (34104, 28482)\n",
      "原始图像数据类型: uint16, 最大值: 10576, 最小值: 0\n",
      "转换后图像形状: (34104, 28482, 1)\n",
      "图像是 uint16 类型，保持原始格式\n",
      "选择的通道: [0]\n",
      "处理后图像形状: (34104, 28482, 1)\n",
      "初始化 CellposeModel (使用默认的 cpsam 模型)...\n",
      "2025-10-23 22:25:34,236 [INFO] ** TORCH CUDA version installed and working. **\n",
      "2025-10-23 22:25:34,237 [INFO] >>>> using GPU (CUDA)\n",
      "2025-10-23 22:25:35,393 [INFO] >>>> loading model C:\\Users\\lyh\\.cellpose\\models\\cpsam\n",
      "运行 Cellpose-SAM 分割，参数如下:\n",
      "- pretrained_model: cpsam (默认)\n",
      "- flow_threshold: 0.4\n",
      "- cellprob_threshold: 0.0\n",
      "- tile_norm_blocksize: 0\n",
      "- batch_size: 2\n",
      "2025-10-23 22:46:46,313 [WARNING] WARNING: image is very large, not using gpu to compute flows from masks for QC step flow_threshold\n",
      "2025-10-23 22:46:46,341 [INFO] turn off QC step with flow_threshold=0 if too slow\n",
      "2025-10-24 01:35:14,981 [WARNING] more than 65535 masks in image, masks returned as np.uint32\n",
      "分割完成。找到 199412 个细胞。\n",
      "掩码文件已保存: C:/Users/lyh/paper/image_baseline_stitch/Xenium_V1_mouse_Colon_FF_exp1\\cellpose_sam_segmentation\\cellpose_sam_masks_Xenium_V1_mouse_Colon_FF.tif\n"
     ]
    }
   ],
   "source": [
    " \n",
    "import os\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "from cellpose import models, core, io, plot\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from natsort import natsorted\n",
    "\n",
    "# Set up logging\n",
    "io.logger_setup()\n",
    "\n",
    "\n",
    "dapi_path = \"C:/Users/lyh/paper/image_baseline_stitch/Xenium_V1_mouse_Colon_FF/dapi/dapi_channel.tif\"\n",
    "\n",
    "# Output directory for segmentation results\n",
    "output_dir_base = \"C:/Users/lyh/paper/image_baseline_stitch/Xenium_V1_mouse_Colon_FF_exp1\"\n",
    "output_dir = os.path.join(output_dir_base, \"cellpose_sam_segmentation\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "dapi_img = tifffile.imread(dapi_path)\n",
    "print(f\"Original image shape: {dapi_img.shape}\")\n",
    "print(f\"Original image dtype: {dapi_img.dtype}, max: {dapi_img.max()}, min: {dapi_img.min()}\")\n",
    "\n",
    "# Image preprocessing - handle possible multi-dimensional images\n",
    "if len(dapi_img.shape) == 3:\n",
    "    # If 3D image (possibly Z, Y, X or Y, X, C)\n",
    "    if dapi_img.shape[0] < dapi_img.shape[1]:  # Channel might be in the first dimension\n",
    "        # Assume (C, H, W) format, convert to (H, W, C)\n",
    "        dapi_img = np.transpose(dapi_img, (1, 2, 0))\n",
    "        print(f\"Converted image format to (H, W, C): {dapi_img.shape}\")\n",
    "    # If multiple Z layers, take maximum projection or middle layer\n",
    "    if dapi_img.shape[-1] > 3:  # If too many channels, might be Z layers\n",
    "        print(\"Detected multi-layer image, using maximum intensity projection\")\n",
    "        dapi_img = np.max(dapi_img, axis=-1, keepdims=True)\n",
    "elif len(dapi_img.shape) == 2:\n",
    "    # If 2D image, convert to 3D (H, W, C)\n",
    "    dapi_img = np.expand_dims(dapi_img, axis=-1)\n",
    "    print(f\"Converted image shape: {dapi_img.shape}\")\n",
    "\n",
    "# Image normalization\n",
    "if dapi_img.max() > 0:\n",
    "    if dapi_img.dtype == np.uint16:\n",
    "        print(\"Image is uint16 type, keeping original format\")\n",
    "        dapi_img_norm = dapi_img.copy()\n",
    "    elif dapi_img.max() > 255:\n",
    "        print(\"Normalizing image to 0-255 (uint8)\")\n",
    "        dapi_img_norm = (dapi_img / dapi_img.max() * 255).astype(np.uint8)\n",
    "    else:\n",
    "        print(\"Image is already in suitable intensity range\")\n",
    "        dapi_img_norm = dapi_img.astype(np.uint8)\n",
    "else:\n",
    "    print(\"Warning: Image max value is 0, might be empty image\")\n",
    "    dapi_img_norm = dapi_img.astype(np.uint8)\n",
    "\n",
    "\n",
    "# Channel selection - according to new Cellpose format\n",
    "first_channel = '0'   # Use the first channel\n",
    "second_channel = 'None'  # No second channel\n",
    "third_channel = 'None'   # No third channel\n",
    "\n",
    "selected_channels = []\n",
    "for i, c in enumerate([first_channel, second_channel, third_channel]):\n",
    "    if c == 'None':\n",
    "        continue\n",
    "    if int(c) >= dapi_img_norm.shape[-1]:  # Fixed boundary check\n",
    "        print(f\"Warning: Channel index {c} exceeds image channel count {dapi_img_norm.shape[-1]}\")\n",
    "        continue\n",
    "    if c != 'None':\n",
    "        selected_channels.append(int(c))\n",
    "\n",
    "print(f\"Selected channels: {selected_channels}\")\n",
    "\n",
    "# Create image with selected channels\n",
    "img_selected_channels = np.zeros_like(dapi_img_norm)\n",
    "if len(selected_channels) > 0:\n",
    "    img_selected_channels[:, :, :len(selected_channels)] = dapi_img_norm[:, :, selected_channels]\n",
    "else:\n",
    "    # If no channels selected, use original image\n",
    "    img_selected_channels = dapi_img_norm\n",
    "\n",
    "print(f\"Processed image shape: {img_selected_channels.shape}\")\n",
    "\n",
    "# Initialize new version of CellposeModel (default using cpsam model)\n",
    "print(\"Initializing CellposeModel (using default cpsam model)...\")\n",
    "model = models.CellposeModel(gpu=True)  # Default pretrained_model=\"cpsam\"\n",
    "\n",
    "# Set segmentation parameters\n",
    "flow_threshold = 0.4     # Flow threshold\n",
    "cellprob_threshold = 0.0 # Cell probability threshold\n",
    "tile_norm_blocksize = 0  # Tile normalization block size, 0 means no tile normalization\n",
    "batch_size = 2           # Reduce batch size to fit large images\n",
    "\n",
    "print(f\"Running Cellpose-SAM segmentation with the following parameters:\")\n",
    "print(f\"- pretrained_model: cpsam (default)\")\n",
    "print(f\"- flow_threshold: {flow_threshold}\")\n",
    "print(f\"- cellprob_threshold: {cellprob_threshold}\")\n",
    "print(f\"- tile_norm_blocksize: {tile_norm_blocksize}\")\n",
    "print(f\"- batch_size: {batch_size}\")\n",
    "\n",
    "# Run new version of Cellpose-SAM segmentation\n",
    "masks, flows, styles = model.eval(\n",
    "    img_selected_channels, \n",
    "    batch_size=batch_size, \n",
    "    flow_threshold=flow_threshold, \n",
    "    cellprob_threshold=cellprob_threshold,\n",
    "    normalize={\"tile_norm_blocksize\": tile_norm_blocksize}\n",
    ")\n",
    "\n",
    "print(f\"Segmentation complete. Found {masks.max()} cells.\")\n",
    "\n",
    "# Save segmentation results\n",
    "mask_filename = \"cellpose_sam_masks_Xenium_V1_mouse_Colon_FF.tif\"\n",
    "mask_path = os.path.join(output_dir, mask_filename)\n",
    "tifffile.imwrite(mask_path, masks.astype(np.uint32))\n",
    "print(f\"Mask file saved: {mask_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc7c2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellpose-sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
